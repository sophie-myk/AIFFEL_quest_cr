{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP81M/VH2DMp33Jx5rkZcsk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sophie-myk/AIFFEL_quest_cr/blob/main/Python/Deep_Learning_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "IQkrM3U8N9Pg"
      },
      "outputs": [],
      "source": [
        "#입력데이터\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 *28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28*28))\n",
        "test_images = test_images.astype(\"float32\") /255"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#모델\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential([\n",
        "     layers.Dense( 512, activation= \"relu\"),\n",
        "     layers.Dense(10, activation= \"softmax\")\n",
        "])"
      ],
      "metadata": {
        "id": "O0AhP2ZZxN3B"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#모델을 컴파일하는 단계\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\", #손실함수\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "HOzk5Lwgxj7r"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#마지막으로, 훈련반복\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CprEPIyfxrr5",
        "outputId": "be9f4ee6-a600-4c85-93dd-1fa7647c82bb"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 11ms/step - accuracy: 0.8721 - loss: 0.4438\n",
            "Epoch 2/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10ms/step - accuracy: 0.9652 - loss: 0.1176\n",
            "Epoch 3/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 12ms/step - accuracy: 0.9778 - loss: 0.0726\n",
            "Epoch 4/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.9843 - loss: 0.0507\n",
            "Epoch 5/5\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9896 - loss: 0.0363\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e65a7d84e10>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1 첫번째 예제를 다시 밑바닥부터 구현\n",
        "import tensorflow as tf\n",
        "\n",
        "#1. NaiveDense 클래스_밀집층(Dense Layer) 구현\n",
        "class NaiveDense:\n",
        "    def __init__(self, input_size, output_size, activation):\n",
        "        self.activation = activation                          # 활성화 함수 설정\n",
        "\n",
        "         # 가중치 초기화\n",
        "        # Initialize weights\n",
        "        w_shape = (input_size, output_size)                 # 가중치 행렬의 크기\n",
        "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
        "        self.W = tf.Variable(w_initial_value)                # 가중치 변수 생성\n",
        "\n",
        "          # 편향 초기화\n",
        "        # Initialize biases\n",
        "        b_shape = (output_size,)\n",
        "        b_initial_value = tf.zeros(b_shape)               # 편향은 0으로 초기화\n",
        "        self.b = tf.Variable(b_initial_value)               # 편향 변수 생성\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "       # 순전파 계산: 입력값 * 가중치 + 편향 -> 활성화 함수 적용\n",
        "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
        "\n",
        "    @property\n",
        "    def weights(self):\n",
        "      # 현재 가중치와 편향 반환\n",
        "        return [self.W, self.b]"
      ],
      "metadata": {
        "id": "02crPrnYya_o"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. NaiveSequential 클래스_다층 네트워크를 구성하기\n",
        "from re import X\n",
        "class NaiveSequential:\n",
        "  def __init__(self,layers):\n",
        "      self.layers = layers            # 레이어 리스트를 저장\n",
        "\n",
        "  def __call__(self,inputs):\n",
        "      x = inputs\n",
        "      for layer in self.layers:       # 각 레이어를 순차적으로 호출\n",
        "        x = layer(x)\n",
        "      return x\n",
        "\n",
        "  @property\n",
        "  def weights(self):\n",
        "     # 모든 레이어의 가중치 반환\n",
        "    weights = []\n",
        "    for layer in self.layers:\n",
        "        weights += layer.weights\n",
        "    return weights"
      ],
      "metadata": {
        "id": "TAEXb2NB1IFf"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8. 모델 생성 및 훈련_NaiveSequential 모델 생성 및 훈련 실행\n",
        "model = NaiveSequential([\n",
        "    NaiveDense(input_size= 28*28, output_size=512, activation=tf.nn.relu),  # 첫 번째 레이어\n",
        "    NaiveDense(input_size= 512, output_size=10, activation=tf.nn.softmax)  # 출력 레이어\n",
        "])\n",
        "assert len(model.weights) == 4"
      ],
      "metadata": {
        "id": "gYrJikIr2U5U"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. BatchGenerator 클래스_데이터를 배치(batch) 단위로 나누기\n",
        "import math\n",
        "\n",
        "class BatchGenerator:\n",
        "  def __init__(self, images, labels, batch_size:128):\n",
        "      assert len(images) == len(labels)  # 이미지와 라벨의 개수가 같아야 함\n",
        "      self.index = 0  # 현재 배치의 시작 인덱스\n",
        "      self.images = images\n",
        "      self.labels = labels\n",
        "      self.batch_size = batch_size\n",
        "      self.num_batches = math.ceil(len(images)/batch_size)  # 총 배치 개수 계산\n",
        "\n",
        "  def next(self):\n",
        "    # 현재 배치 데이터 추출\n",
        "    images = self.images[self.index : self.index + self.batch_size]\n",
        "    labels = self.labels[self.index : self.index + self.batch_size]\n",
        "    self.index += self.batch_size  # 인덱스를 다음 배치로 이동\n",
        "    return images,labels"
      ],
      "metadata": {
        "id": "UHp1ChST4Ncn"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4. 훈련 스텝 정의_단일 배치에 대해 모델을 훈련\n",
        "\n",
        "def one_training_step(model, images_batch, labels_batch):\n",
        "    with tf.GradientTape() as tape: # 기울기 계산을 위해 GradientTape 사용\n",
        "      predictions = model(images_batch) # 모델의 예측값\n",
        "      per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(labels_batch, predictions)\n",
        "      average_loss = tf.reduce_mean(per_sample_losses) # 손실의 평균 계산\n",
        "    gradients = tape.gradient(average_loss, model.weights) # 가중치에 대한 기울기 계산\n",
        "    update_weights(gradients, model.weights) # 가중치 업데이트\n",
        "    return average_loss"
      ],
      "metadata": {
        "id": "fzZOb3Rd6hHt"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5. 가중치 업데이트_손실에 따라 가중치를 업데이트\n",
        "learning_rate = 1e-3\n",
        "\n",
        "def update_weights(gradients, weights):\n",
        "  for g, w in zip(gradients, weights): # 각 가중치와 기울기를 차례로 꺼냄\n",
        "    w.assign_sub(g * learning_rate)    # 기울기 반영하여 가중치 업데이트"
      ],
      "metadata": {
        "id": "QN38_o0s7suB"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import optimizers\n",
        "\n",
        "# Change the name to 'optimizer' to avoid confusion\n",
        "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "def update_weights(gradients, weights):\n",
        "    # Now you can use 'optimizer' here\n",
        "    optimizer.apply_gradients(zip(gradients, weights))"
      ],
      "metadata": {
        "id": "yH1OD16j8DDg"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6. 훈련 루프 구현_전체 데이터셋에 대해 에포크 반복\n",
        "def fit(model, images, labels, epochs, batch_size=128):\n",
        "    for epoch_counter in range(epochs):\n",
        "        print(f\"에포크 {epoch_counter}\")\n",
        "        batch_generator = BatchGenerator(images, labels, batch_size)\n",
        "        for batch_counter in range(batch_generator.num_batches):\n",
        "            images_batch, labels_batch = batch_generator.next()\n",
        "            loss = one_training_step(model, images_batch, labels_batch)  # 배치 훈련\n",
        "            if batch_counter % 100 == 0:\n",
        "                print(f\"{batch_counter}번째 손실: {loss:.2f}\")"
      ],
      "metadata": {
        "id": "MAfUKJUf8wzX"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. MNIST 데이터셋 준비_데이터 전처리\n",
        "from tensorflow.keras.datasets import mnist\n",
        "# 데이터 로드\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# 데이터 형태 변경 및 정규화\n",
        "train_images = train_images.reshape((60000, 28 *28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28*28))\n",
        "test_images = test_images.astype(\"float32\") /255\n",
        "\n",
        "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LI7o15a-RTh",
        "outputId": "da24cde9-847c-4b5a-d5a4-6b171fb35331"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "에포크 0\n",
            "0번째 손실: 6.13\n",
            "100번째 손실: 2.23\n",
            "200번째 손실: 2.20\n",
            "300번째 손실: 2.07\n",
            "400번째 손실: 2.20\n",
            "에포크 1\n",
            "0번째 손실: 1.85\n",
            "100번째 손실: 1.86\n",
            "200번째 손실: 1.82\n",
            "300번째 손실: 1.69\n",
            "400번째 손실: 1.80\n",
            "에포크 2\n",
            "0번째 손실: 1.53\n",
            "100번째 손실: 1.56\n",
            "200번째 손실: 1.49\n",
            "300번째 손실: 1.41\n",
            "400번째 손실: 1.48\n",
            "에포크 3\n",
            "0번째 손실: 1.28\n",
            "100번째 손실: 1.33\n",
            "200번째 손실: 1.23\n",
            "300번째 손실: 1.20\n",
            "400번째 손실: 1.24\n",
            "에포크 4\n",
            "0번째 손실: 1.09\n",
            "100번째 손실: 1.15\n",
            "200번째 손실: 1.04\n",
            "300번째 손실: 1.04\n",
            "400번째 손실: 1.08\n",
            "에포크 5\n",
            "0번째 손실: 0.95\n",
            "100번째 손실: 1.01\n",
            "200번째 손실: 0.90\n",
            "300번째 손실: 0.93\n",
            "400번째 손실: 0.97\n",
            "에포크 6\n",
            "0번째 손실: 0.85\n",
            "100번째 손실: 0.91\n",
            "200번째 손실: 0.80\n",
            "300번째 손실: 0.84\n",
            "400번째 손실: 0.88\n",
            "에포크 7\n",
            "0번째 손실: 0.77\n",
            "100번째 손실: 0.82\n",
            "200번째 손실: 0.72\n",
            "300번째 손실: 0.77\n",
            "400번째 손실: 0.82\n",
            "에포크 8\n",
            "0번째 손실: 0.71\n",
            "100번째 손실: 0.76\n",
            "200번째 손실: 0.66\n",
            "300번째 손실: 0.71\n",
            "400번째 손실: 0.77\n",
            "에포크 9\n",
            "0번째 손실: 0.67\n",
            "100번째 손실: 0.70\n",
            "200번째 손실: 0.61\n",
            "300번째 손실: 0.67\n",
            "400번째 손실: 0.73\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9. 모델 평가_ 정확도 계산\n",
        "import numpy as np # Add this line to import numpy\n",
        "\n",
        "predictions = model(test_images)  # 테스트 데이터 예측\n",
        "predictions = predictions.numpy()\n",
        "predicted_labels = np.argmax(predictions, axis=1) # 가장 높은 확률의 클래스를 선택\n",
        "matches = predicted_labels == test_labels\n",
        "print(f\"정확도: {matches.mean():.2f}\")            # 정확도 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k437q8cBjee",
        "outputId": "99f9577d-18c2-4b91-b306-cb6927981765"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "정확도: 0.81\n"
          ]
        }
      ]
    }
  ]
}